{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a919d70b91f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Keras General\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m    \u001b[1;31m#initialize NN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCallback\u001b[0m   \u001b[1;31m#save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "#General\n",
    "import numpy as np    \n",
    "import pandas as pd\n",
    "\n",
    "#Keras General\n",
    "from keras.models import Sequential    #initialize NN\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback   #save model\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint #choose best model\n",
    "from keras import backend as K              #create new metrics\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "\n",
    "#Keras layers\n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D,AveragePooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers.normalization import BatchNormalization #normalization\n",
    "\n",
    "#other technique\n",
    "from keras import optimizers         #optimizer\n",
    "from keras.optimizers import Adam    #Adam optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHODOLOGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWe have just 64 football images. Problem is a Classification.\\nSo I need of two classes:\\n- Class 0: Football\\n- Class 1: General\\n\\nSo I insert 468 random picture from google \\n(101_ObjectCategories/BACKGROUD_GOOGLE).\\nThen I download others 171 football pictures from Google Image.\\n\\nSo I had an dataset of 539 pictures wi\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "There are only 64 picture of football ball. \n",
    "Problem is a simple classification with Deep Learning.\n",
    "\n",
    "For this are required two classes:\n",
    "- Class 0: Football ball\n",
    "- Class 1: General\n",
    "\n",
    "For class 1, I used 468 random picture take from google.\n",
    "Instead for class 0, I downloaded other 171 football ball pictures \n",
    "from Google Image. (I also used pictures not of football ball but\n",
    "of things with drawn football ball, for example pillow.)\n",
    "\n",
    "So the dataset is with 539 pictures where 171 are class 0 and \n",
    "468 are class 1.\n",
    "Where 60% will be training and 40% testing.\n",
    "\n",
    "Because them are few, for increasing the size of dataset I used data\n",
    "augmentation.\n",
    "\n",
    "The architecture of Deep learning is a CNN with 3 Layer of Convolutional\n",
    "layer with maxpooling and a ANN.\n",
    "\n",
    "Images have different size, so I resized their to width and height 100x100.\n",
    "Each picture is loaded on a tensor 100x100x3, because their are colored\n",
    "and I used three channel ('rgb'). Can see this as 539x3 matrixs with\n",
    "100 rows and 100 columns where each coordinate (x,y) indicate a pixel\n",
    "with value from 0 to 255 for each channel.\n",
    "Input will be (before data augmentantion) of 539x100x100x3.\n",
    "\n",
    "This is big! The role of CNN is to reduce the images without \n",
    "losing information.\n",
    "\n",
    "I chose to use a batch size of 32 samples for propagated through \n",
    "the network. This mean take the first 32 samples and train a CNN and\n",
    "will update the weights, the will be take the following 32 samples and\n",
    "will train again the CNN updating again the weights. \n",
    "\n",
    "\n",
    "Instead learning rate is a hyper-parameter that control how much are adjusting \n",
    "weights.\n",
    "\n",
    "To extract features of a image, my CNN uses a kernel 3x3 with\n",
    "no stride (we can image it as a little matrix 3x3 that move on each image). \n",
    "They use different filters on each ConvLayer and take the most important\n",
    "feature with Maxpooling (2x2). So a filter is a little matrix 3x3 with usually\n",
    "value -1,0,1... that will be multiply with a part of the image and will be\n",
    "take for last the biggest value. This for each filter.\n",
    "Will apply a relu function.\n",
    "\n",
    "\n",
    "The Fully-Connected layer with Flatten is learning a possibly \n",
    "non-linear function in that space. The model is able\n",
    "to see the dominating features in image and classify them\n",
    "using the Softmax Classification.\n",
    "\n",
    "I worked on CPU with just 10 epoches with optimizer Adam and\n",
    "with loss function \"categorical_crossentropy\".\n",
    "\n",
    "I obtained an accuracy of more 0.9.\n",
    "\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 450 images belonging to 2 classes.\n",
      "Found 188 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "7000/7000 [==============================] - 2281s 326ms/step - loss: 0.0419 - acc: 0.9841 - val_loss: 0.5524 - val_acc: 0.8777\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55243, saving model to model_best_weights.h5\n",
      "Epoch 2/10\n",
      "7000/7000 [==============================] - 2257s 322ms/step - loss: 0.0072 - acc: 0.9983 - val_loss: 0.5239 - val_acc: 0.9255\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55243 to 0.52394, saving model to model_best_weights.h5\n",
      "Epoch 3/10\n",
      "7000/7000 [==============================] - 2265s 324ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.5897 - val_acc: 0.9043\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52394\n",
      "Epoch 4/10\n",
      "7000/7000 [==============================] - 2271s 324ms/step - loss: 0.0069 - acc: 0.9987 - val_loss: 0.5887 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52394\n",
      "Epoch 5/10\n",
      "7000/7000 [==============================] - 30654s 4s/step - loss: 0.0044 - acc: 0.9992 - val_loss: 0.7010 - val_acc: 0.9255\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52394\n",
      "Epoch 6/10\n",
      "7000/7000 [==============================] - 2663s 380ms/step - loss: 0.0080 - acc: 0.9986 - val_loss: 0.6594 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.52394\n",
      "Epoch 7/10\n",
      "7000/7000 [==============================] - 2508s 358ms/step - loss: 0.0038 - acc: 0.9993 - val_loss: 1.0371 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.52394\n",
      "Epoch 8/10\n",
      "7000/7000 [==============================] - 2489s 356ms/step - loss: 0.0065 - acc: 0.9991 - val_loss: 0.6616 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.52394\n",
      "Epoch 9/10\n",
      "7000/7000 [==============================] - 2462s 352ms/step - loss: 0.0077 - acc: 0.9991 - val_loss: 0.9399 - val_acc: 0.9362\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.52394\n",
      "Epoch 10/10\n",
      "7000/7000 [==============================] - 2384s 341ms/step - loss: 0.0103 - acc: 0.9988 - val_loss: 0.9807 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.52394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22500ca2780>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "num_filters = 16\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "larg=100\n",
    "lung=100\n",
    "ch=3\n",
    "f_1='relu'\n",
    "f_2='softmax'\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(num_filters*4, filter_size, input_shape=(width, height, ch),activation=f_1))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Conv2D(num_filters*2, filter_size,activation=f_1))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Conv2D(num_filters, filter_size,activation=f_1))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(layers.Dropout(0.60))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=f_1))\n",
    "model.add(Dense(2, activation=f_2))\n",
    "\n",
    "#applichiamo delle traformazioni casuacategorical_crossentropyli per aumentare il numero di esempi nel training\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(r'...\\training',\n",
    "                                            target_size = (width, height),\n",
    "                                            batch_size = n_batch,\n",
    "                                            color_mode='rgb',\n",
    "                                            classes = ['Soccer','Google'],\n",
    "                                           class_mode = 'categorical',\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(r'...\\testing',\n",
    "                                            target_size = (width, height),\n",
    "                                            batch_size = n_batch,\n",
    "                                            color_mode='rgb',\n",
    "                                            classes = ['Soccer','Google'],\n",
    "                                           class_mode = 'categorical',\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.002)\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "#early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=3, mode='min', verbose=1)\n",
    "checkpoint = ModelCheckpoint('model_best_weights.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min', period=1)\n",
    "\n",
    "\n",
    "model.fit_generator(training_set,\n",
    "                         steps_per_epoch = train_dim,   #numero di immagini nel trainig set (tutte le osservazioni vengono passate durante ogni epoca)\n",
    "                         epochs = n_epoches,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = test_dim,verbose=1,callbacks = [checkpoint])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate on original images\n",
    "\n",
    "model=load_model(\"model_best_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "x=range(1,65)    # there are 64 images in folder\n",
    "L=[]             # save image names in this list\n",
    "for k in x:\n",
    "    L.append('image_'+str(k).zfill(4)+'.jpg') #use lpad to generate name\n",
    "\n",
    "pred=0\n",
    "n=0\n",
    "for li in L:\n",
    "    fold='SoccerBall'\n",
    "    img=li\n",
    "    test_image = image.load_img('.../'+img, target_size = (larg, lung),grayscale=False)\n",
    "\n",
    "    test_image = image.img_to_array(test_image)\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis = 0)/255\n",
    "    if model.predict(test_image)[0][0]>0.5:\n",
    "        pred+=1\n",
    "\n",
    "    \n",
    "tp=pred/len(L)\n",
    "tp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROVA RETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model(\"model_best_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
